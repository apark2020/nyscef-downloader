import argparse
from datetime import date, timedelta
import re
import requests
import sys

from bs4 import BeautifulSoup

from utils.courts import COURTS

MAX_REQUEST_PER_SESSION = 90
CAPTCHA_ERROR = "The answer that was given did not match the text in the picture. Please try again."
NYSCEF_BASE_URL = "https://iapps.courts.state.ny.us/nyscef/"


def get_new_session_id():
    print('You need to provide a new JSESSIONID:')
    session_id = input().strip()
    return session_id


def daterange(start_date, end_date):
    for n in range(int((end_date - start_date).days)):
        yield start_date + timedelta(n)


def write_data_to_file(link, output_file):
    url = link.findAll('a')[0].get('href')
    docket_id = re.search(r'(?:docketId)=(.*)(?:==)', url).group(0).replace('docketId=', '').replace('==', '') # noqa
    with open(output_file, "a") as ids_file:
        ids_file.write(docket_id)
        ids_file.write("\n")


def check_for_error(soup):
    if soup.find(class_="MsgBox_Error"):
        error_text = soup.find("span", class_="MsgBox_Message").text.strip()
        if error_text == CAPTCHA_ERROR:
            sys.exit("Error around captcha input!")


def main(start_date, initial_session_id, case_type, end_date, output_file, selected_court):
    request_headers = {"User-Agent": ""}
    request_params = (("TAB", "courtDateRange"),)
    request_data = {"btnSubmit": "Search"}
    request_cookies = {"JSESSIONID": initial_session_id}
    request_count = 0

    for filing_date in daterange(start_date, end_date):
        filing_date = filing_date.strftime("%m/%d/%Y")
        print("Starting filing date {}".format(filing_date))
        request_data['txtFilingDate'] = filing_date

        # If the user has a selected court, use that, else use our whole list of courts
        if selected_court:
            ny_courts = {selected_court: COURTS[selected_court]}
        else:
            ny_courts = COURTS

        for court, court_id in ny_courts.items():
            print("Starting court: {}".format(court))
            request_data['selCountyCourt'] = court_id
            initial_response = requests.post(
                NYSCEF_BASE_URL + 'CaseSearch?PageNum=1',
                headers=request_headers,
                params=request_params,
                cookies=request_cookies,
                data=request_data
            )
            request_count += 1

            # If the request count has exceeded our experimental max requests per session constant,
            # we'll want to reset the count and get a new cookie generated by the user
            if request_count > MAX_REQUEST_PER_SESSION:
                request_cookies['JSESSIONID'] = get_new_session_id()
                request_count = 0

            # Sometimes there are errors having to do with a bad input or a bad JSESSIONID. Here we
            # output them.
            soup = BeautifulSoup(initial_response.text, 'html.parser')
            check_for_error(soup)

            # We want to iterate across all pages in our results, so we are grabbing the page
            # count from the first initial result here. If the pages element is not present, i.e.
            # we have an AttributeError, we'll just set to 1.
            try:
                last_page = [
                    a.get('href') for a in soup.find('span', class_='pageNumbers')
                    .findAll('a')
                ][-1]
                pages_num = int(last_page.split('?PageNum=')[-1])
                print('Pages: {}'.format(pages_num))
            except AttributeError:
                pages_num = 1

            # Now that we have the full number of pages, iterate through each page number and make
            # a request for the data on that page.
            for i in range(0, pages_num):
                request_params = (('PageNum', str(i + 1)),)
                page_response = requests.get(
                    NYSCEF_BASE_URL + 'CaseSearchResults',
                    headers=request_headers,
                    params=request_params,
                    cookies=request_cookies,
                    data=request_data
                )
                request_count += 1
                soup = BeautifulSoup(page_response.text, 'html.parser')
                check_for_error(soup)

                # We iterate through the table data by each row in the results.
                rows = soup.findAll('tr')
                for row in rows:
                    table_data = row.findAll('td')
                    if table_data:
                        link = table_data[0]
                        stated_case_type = table_data[3]
                        # If a case type is specified only grab that type, if not we write every id
                        # to the output_file
                        if case_type in stated_case_type.text:
                            write_data_to_file(link, output_file)


if __name__ == '__main__':
    cl = argparse.ArgumentParser(description="This script finds docket ids on NYSCEF.")
    cl.add_argument("--start-date", required=True, help="the date you want to start as dd/mm/yyyy")
    cl.add_argument("--session-id", required=True, help="the initial JSESSIONID cookie generated")
    cl.add_argument("--case-type", required=True, help="the case type you are searching for")
    cl.add_argument("--end-date", default=date.today(), help="the end date for the search")
    cl.add_argument("--output", default="ids.txt", help="file we write the ids to")
    cl.add_argument("--court", help="court where the cases are you want to find")
    args = cl.parse_args()

    if args.court and args.court not in COURTS.keys():
        sys.exit("Court specified not found")

    month, day, year = args.start_date.split('/')
    start_date = date(int(year), int(month), int(day))

    sys.exit(main(
        start_date,
        args.session_id,
        args.case_type,
        args.end_date,
        args.output,
        args.court
    ))
