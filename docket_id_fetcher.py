import argparse
from datetime import date, timedelta
import re
import requests
import sys

from bs4 import BeautifulSoup

from utils.courts import COURTS

MAX_REQUEST_PER_SESSION = 60
NYSCEF_BASE_URL = "https://iapps.courts.state.ny.us/nyscef/"


def get_new_session_id():
    print('You need to provide a new JSESSIONID:')
    session_id = input().strip()
    return session_id



def write_data_to_file(link, output_file):
    url = link.findAll('a')[0].get('href')
    docket_id = re.search(r'(?:docketId)=(.*)(?:==)', url).group(0).replace('docketId=', '').replace('==', '') # noqa
    with open(output_file, "a") as ids_file:
        ids_file.write(docket_id)
        ids_file.write("\n")


def main(case_id, initial_session_id, output_file, selected_court):
    request_headers = {"User-Agent": ""}
    request_params = (("TAB", "courtDateRange"),)
    request_data = {"btnSubmit": "Search"}
    request_cookies = {"JSESSIONID": initial_session_id}
    request_count = 0


    request_data['txtCaseIdentifierNumber'] = case_id.strip()

    # If the user has a selected court, use that, else use our whole list of courts
    if selected_court:
        ny_courts = {selected_court: COURTS[selected_court]}
    else:
        ny_courts = COURTS

    for court, court_id in ny_courts.items():
        print("Starting court: {}".format(court))
        request_data['selCountyCourt'] = court_id
        initial_response = requests.post(
            NYSCEF_BASE_URL + 'CaseSearch?PageNum=1',
            headers=request_headers,
            params=request_params,
            cookies=request_cookies,
            data=request_data
        )
        request_count += 1

        # If the request count has exceeded our experimental max requests per session constant,
        # we'll want to reset the count and get a new cookie generated by the user
        if request_count > MAX_REQUEST_PER_SESSION:
            request_cookies['JSESSIONID'] = get_new_session_id()
            request_count = 0

        # Sometimes there are errors having to do with a bad input or a bad JSESSIONID. Here we
        # output them.
        soup = BeautifulSoup(initial_response.text, 'html.parser')

        # Now that we have the full number of pages, iterate through each page number and make
        # a request for the data on that page.
        request_params = (('PageNum', "1"),)
        page_response = requests.get(
            NYSCEF_BASE_URL + 'CaseSearchResults',
            headers=request_headers,
            params=request_params,
            cookies=request_cookies,
            data=request_data
        )
        request_count += 1
        soup = BeautifulSoup(page_response.text, 'html.parser')

        # We iterate through the table data by each row in the results.
        rows = soup.findAll('tr')
        for row in rows:
            table_data = row.findAll('td')
            if table_data:
                link = table_data[0]
                write_data_to_file(link, output_file)


if __name__ == '__main__':
    cl = argparse.ArgumentParser(description="This script finds docket ids on NYSCEF.")
    cl.add_argument("--case-id", required=True, help="input the case id")
    cl.add_argument("--session-id", required=True, help="the initial JSESSIONID cookie generated")
    cl.add_argument("--output", default="ids1.txt", help="file we write the ids to")
    cl.add_argument("--court", help="court where the cases are you want to find")
    args = cl.parse_args()

    if args.court and args.court not in COURTS.keys():
        sys.exit("Court specified not found")

    sys.exit(main(
        args.case_id,
        args.session_id,
        args.output,
        args.court
    ))
