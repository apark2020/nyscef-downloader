import argparse
from datetime import date, timedelta
import re
import requests
import sys

from bs4 import BeautifulSoup

from utils.courts import COURTS

MAX_REQUEST_PER_SESSION = 60
NYSCEF_BASE_URL = "https://iapps.courts.state.ny.us/nyscef/"


def get_new_session_id():
    print('You need to provide a new JSESSIONID. Go to https://iapps.courts.state.ny.us/nyscef/CaseSearch?TAB=caseIdentifier, complete the captcha, then enter the JSESSIONID under Inspect > Applications:')
    session_id = input().strip()
    return session_id



def write_data_to_file(link, output_file):
    url = link.findAll('a')[0].get('href')
    docket_id = re.search(r'(?:docketId)=(.*)(?:==)', url).group(0).replace('docketId=', '').replace('==', '') # noqa
    with open(output_file, "a") as ids_file:
        ids_file.write(docket_id)
        ids_file.write("\n")


def main(case_ids, initial_session_id, output_file):
    case_counter=1
    request_count=0
    for caseid in case_ids:
        print("Case "+str(case_counter)+" out of "+str(len(case_ids)))
        request_headers = {"User-Agent": ""}
        request_params = (("TAB", "caseIdentifier"),)
        request_data = {"btnSubmit": "Search"}
        request_cookies = {"JSESSIONID": initial_session_id}

        request_data['rbCaseIdentifierNumberType'] = "indexNumber"
        request_data['txtCaseIdentifierNumber'] = caseid.strip()

        request_data['txtCounty'] = "-1"
        initial_response = requests.post(
            NYSCEF_BASE_URL + 'CaseSearch?PageNum=1',
            headers=request_headers,
            params=request_params,
            cookies=request_cookies,
            data=request_data
        )
        request_count += 1

        # If the request count has exceeded our experimental max requests per session constant,
        # we'll want to reset the count and get a new cookie generated by the user
        if request_count > MAX_REQUEST_PER_SESSION:
            request_cookies['JSESSIONID'] = get_new_session_id()
            request_count = 0

        # Sometimes there are errors having to do with a bad input or a bad JSESSIONID. Here we
        # output them.
        soup = BeautifulSoup(initial_response.text, 'html.parser')

        # Now that we have the full number of pages, iterate through each page number and make
        # a request for the data on that page.
        request_params = (('PageNum', "1"),)
        page_response = requests.get(
            NYSCEF_BASE_URL + 'CaseSearchResults',
            headers=request_headers,
            params=request_params,
            cookies=request_cookies,
            data=request_data
        )
        request_count += 1
        soup = BeautifulSoup(page_response.text, 'html.parser')

        # We iterate through the table data by each row in the results.
        rows = soup.findAll('tr')
        for row in rows:
            table_data = row.findAll('td')
            if table_data:
                link = table_data[0]
                write_data_to_file(link, output_file)
        case_counter+=1


if __name__ == '__main__':
    cl = argparse.ArgumentParser(description="This script finds docket ids on NYSCEF.")
    cl.add_argument("--case-id", help="input the case ID")
    cl.add_argument("--case-id-file", help="input the case ID file")
    cl.add_argument("--session-id", required=True, help="the initial JSESSIONID cookie generated")
    cl.add_argument("--output", default="ids1.txt", help="file we write the ids to")
    args = cl.parse_args()

    if args.case_id:
        case_ids=args.case_id.strip()
    else:
        with open(args.case_id_file) as f:
            case_ids = f.readlines()

    sys.exit(main(
        case_ids,
        args.session_id,
        args.output,
    ))
